import streamlit as st

from langchain.retrievers import ParentDocumentRetriever
from langchain_text_splitters import RecursiveCharacterTextSplitter
#from langchain_chroma import Chroma
from langchain_community.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.storage._lc_store import create_kv_docstore
from faiss import IndexFlatL2
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores.utils import DistanceStrategy
from tqdm import tqdm
from langchain_core.runnables import chain
from typing import List, Any, Dict, Optional
from langchain_core.documents import Document
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from collections import defaultdict

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.utils.function_calling import convert_to_openai_function
from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser
from langchain_core.runnables import RunnableLambda
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser
from langchain.prompts import MessagesPlaceholder
from langchain.agents import AgentExecutor
from langchain.agents.format_scratchpad import format_to_openai_functions
from typing import Literal, List
import os


import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

api_key = openai_key = "xxxxxxxxxxxx"

# ====================================================================
# Load databases and initialize the retrievers
# ====================================================================
# Instantiate the OpenAIEmbeddings class
openai_embeddings = OpenAIEmbeddings(api_key=api_key)
# This text splitter is used to create the parent documents
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
# This text splitter is used to create the child documents
# It should create documents smaller than the parent
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

# https://python.langchain.com/v0.2/docs/how_to/add_scores_retriever/

import PyPDF2
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import os

# def process_and_add_pdf(uploaded_file=None, pdf_path=None):
#     # Extrair texto do PDF
#     logger.info("üîÑ Iniciando processamento do PDF...")
#     try:
#         if uploaded_file:
#             logger.info(f"üì§ Processando arquivo enviado pelo usu√°rio")
#             pdf_reader = PyPDF2.PdfReader(uploaded_file)
#         elif pdf_path:
#             logger.info(f"üìÇ Processando arquivo do caminho: {pdf_path}")
#             with open(pdf_path, "rb") as f:
#                 pdf_reader = PyPDF2.PdfReader(f)
#                 text = ""
#                 for page in pdf_reader.pages:
#                     text += page.extract_text()
#         else:
#             raise ValueError("No PDF file provided.")
#         logger.info(f"üìÑ Texto extra√≠do com sucesso. Tamanho: {len(text)} caracteres")

#         # Dividir o texto em chunks e resumos
#         parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
#         child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

#         parent_chunks = parent_splitter.split_text(text)
#         child_chunks = child_splitter.split_text(text)

#         # Criar documentos para indexa√ß√£o
#         parent_docs = [Document(page_content=chunk) for chunk in parent_chunks]
#         child_docs = [Document(page_content=chunk) for chunk in child_chunks]

#         # Indexar os documentos usando FAISS
#         vectorstore = FAISS.from_documents(parent_docs, openai_embeddings)
#         vectorstore.save_local("/home/romulobrito/projetos/image-RAG/text_image_rag/indices/chunks/chunk_vectorstore_marco")

#         summary_vectorstore = FAISS.from_documents(child_docs, openai_embeddings)
#         summary_vectorstore.save_local("/home/romulobrito/projetos/image-RAG/text_image_rag/indices/summaries/whole_doc_summary_vectorstore_marco")

#         logger.info(f"‚úÇÔ∏è Chunks gerados: {len(parent_chunks)} parent, {len(child_chunks)} child")
#         return vectorstore, summary_vectorstore
#     except Exception as e:
#         logger.error(f"‚ùå Erro no processamento do PDF: {e}")
#         raise

def process_and_add_pdf(uploaded_file=None, pdf_path=None):
    # Definir caminhos dos √≠ndices
    base_path = "/home/romulobrito/projetos/image-RAG/text_image_rag/indices"
    chunks_path = f"{base_path}/chunks/chunk_vectorstore_marco"
    summaries_path = f"{base_path}/summaries/whole_doc_summary_vectorstore_marco"
    
    # Criar diret√≥rios se n√£o existirem
    for path in [f"{base_path}/chunks", f"{base_path}/summaries"]:
        os.makedirs(path, exist_ok=True)
    
    logger.info("üîÑ Iniciando processamento do PDF...")
    try:
        if uploaded_file:
            logger.info(f"üì§ Processando arquivo enviado pelo usu√°rio")
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
        elif pdf_path:
            logger.info(f"üìÇ Processando arquivo do caminho: {pdf_path}")
            with open(pdf_path, "rb") as f:
                pdf_reader = PyPDF2.PdfReader(f)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text()
        else:
            raise ValueError("No PDF file provided.")
        
        logger.info(f"üìÑ Texto extra√≠do com sucesso. Tamanho: {len(text)} caracteres")

        # Dividir o texto em chunks e resumos
        parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
        child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

        parent_chunks = parent_splitter.split_text(text)
        child_chunks = child_splitter.split_text(text)

        # Criar documentos para indexa√ß√£o
        parent_docs = [Document(page_content=chunk) for chunk in parent_chunks]
        child_docs = [Document(page_content=chunk) for chunk in child_chunks]

        logger.info(f"‚úÇÔ∏è Chunks gerados: {len(parent_chunks)} parent, {len(child_chunks)} child")

        # Indexar os documentos usando FAISS
        vectorstore = FAISS.from_documents(parent_docs, openai_embeddings)
        vectorstore.save_local(chunks_path)
        logger.info(f"üíæ √çndice de chunks salvo em: {chunks_path}")

        summary_vectorstore = FAISS.from_documents(child_docs, openai_embeddings)
        summary_vectorstore.save_local(summaries_path)
        logger.info(f"üíæ √çndice de resumos salvo em: {summaries_path}")

        return vectorstore, summary_vectorstore
    except Exception as e:
        logger.error(f"‚ùå Erro no processamento do PDF: {e}")
        raise

class CustomParentDocumentRetriever:
    def __init__(self, vectorstore, docstore, child_splitter, parent_splitter, search_kwargs):
        self.vectorstore = vectorstore
        self.docstore = docstore
        self.child_splitter = child_splitter
        self.parent_splitter = parent_splitter
        self.search_kwargs = search_kwargs

    def retrieve(self, query: str) -> List[Document]:
        """Retrieve relevant documents based on the query."""
        results = self.vectorstore.similarity_search_with_score(query, **self.search_kwargs)
        id_to_doc = defaultdict(list)
        for doc, score in results:
            doc_id = doc.metadata.get("doc_id")
            if doc_id:
                doc.metadata["score"] = score
                id_to_doc[doc_id].append(doc)

        docs = []
        for _id, sub_docs in id_to_doc.items():
            docstore_docs = self.docstore.mget([_id])
            if docstore_docs:
                if doc := docstore_docs[0]:
                    doc.metadata["sub_docs"] = sub_docs
                    docs.append(doc)

        return docs



# def load_chunk_retriever() -> CustomParentDocumentRetriever:
#     vectorstore = FAISS.load_local(
#         "/home/romulobrito/projetos/image-RAG/text_image_rag/indices/chunks/chunk_vectorstore_marco",
#         openai_embeddings,
#         allow_dangerous_deserialization=True
#     )
#     fs = LocalFileStore("/home/romulobrito/projetos/image-RAG/text_image_rag/indices/chunks/chunk_docstore_marco")
#     store = create_kv_docstore(fs)
#     return CustomParentDocumentRetriever(
#         vectorstore=vectorstore,
#         docstore=store,
#         child_splitter=child_splitter,
#         parent_splitter=parent_splitter,
#         search_kwargs={'score_threshold': 0.5, 'k': 4}
#     )


def load_chunk_retriever():
    logger.info("üîÑ Carregando/Inicializando retrievers...")
    
    base_path = "/home/romulobrito/projetos/image-RAG/text_image_rag/indices"
    chunks_path = f"{base_path}/chunks/chunk_vectorstore_marco"
    index_file = os.path.join(chunks_path, "index.faiss")
    
    # Criar diret√≥rios se n√£o existirem
    os.makedirs(chunks_path, exist_ok=True)
    
    # Verificar se o arquivo de √≠ndice existe
    if not os.path.exists(index_file):
        logger.info("üîÑ √çndice n√£o encontrado. Criando novo √≠ndice...")
        
        # Criar o √≠ndice a partir do PDF padr√£o
        pdf_path = "/home/romulobrito/projetos/image-RAG/text_image_rag/Injection Mold Design Handbook - preview-9781569908167_A42563111.pdf"
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDF n√£o encontrado em: {pdf_path}")
            
        vectorstore, _ = process_and_add_pdf(pdf_path=pdf_path)
        logger.info("‚úÖ Novo √≠ndice criado com sucesso")
    else:
        # Carregar √≠ndice existente com allow_dangerous_deserialization=True
        vectorstore = FAISS.load_local(
            folder_path=chunks_path,
            embeddings=openai_embeddings,
            index_name="index",
            allow_dangerous_deserialization=True  # Adicionado este par√¢metro
        )
        logger.info("‚úÖ √çndice FAISS carregado com sucesso")

    # Configurar o retriever
    search_kwargs = {"k": 5}
    return vectorstore.as_retriever(search_kwargs=search_kwargs)

def load_summary_retriever() -> chain:
    vectorstore = FAISS.load_local(
        "/home/romulobrito/projetos/image-RAG/text_image_rag/indices/summaries/whole_doc_summary_vectorstore_marco",
        openai_embeddings,
        allow_dangerous_deserialization=True
    )

    @chain
    def whole_doc_summary_retriever(query: str) -> List[Document]:
        docs, scores = zip(*vectorstore.similarity_search_with_score(query=query, k=2))
        for doc, score in zip(docs, scores):
            doc.metadata["score"] = score
        return docs

    return whole_doc_summary_retriever

# No in√≠cio do arquivo, junto com os outros imports
from PIL import Image
from sentence_transformers import SentenceTransformer, util
import torch
import numpy as np
from typing import List, Tuple, Any, Dict
import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.documents import Document
from pathlib import Path

# Cache do modelo de similaridade para evitar recarregamento
@st.cache_resource
def get_similarity_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

def describe_and_rank_images(query: str, answer: Any, image_dir: str, n: int = 3) -> List[Tuple[str, str, float]]:
    """
    Retorna os caminhos das imagens mais relevantes junto com suas descri√ß√µes e relev√¢ncia.
    
    Args:
        query (str): A pergunta do usu√°rio
        answer (Any): A resposta gerada pelo sistema (pode ser string ou AIMessage)
        image_dir (str): Diret√≥rio onde est√£o as imagens
        n (int): N√∫mero m√°ximo de imagens para retornar
        
    Returns:
        List[Tuple[str, str, float]]: Lista de tuplas (caminho_imagem, descri√ß√£o, relev√¢ncia)
    """
    logger.info(f"üñºÔ∏è Iniciando processamento de imagens para query: {query[:50]}...")

    try:
        # Extrair texto da resposta se for AIMessage
        if hasattr(answer, 'content'):
            answer_text = answer.content
        else:
            answer_text = str(answer)

        # Verificar se o diret√≥rio existe
        image_dir = Path(image_dir)
        if not image_dir.exists():
            print(f"Diret√≥rio de imagens n√£o encontrado: {image_dir}")
            return []
        
        # Inicializar o modelo para descri√ß√£o de imagens
        description_llm = ChatOpenAI(temperature=0.3, model='gpt-4', openai_api_key=api_key)
        
        # Obter modelo de similaridade do cache
        similarity_model = get_similarity_model()
        
        # Template para gerar descri√ß√µes t√©cnicas das imagens
        description_template = """Voc√™ √© um especialista em moldes de inje√ß√£o pl√°stica.
        Analise a imagem e gere uma descri√ß√£o t√©cnica detalhada do que ela provavelmente representa dentro do contexto da resposta e da query fornecida.
        
        Nome do arquivo: {image_name}
        
        Gere uma descri√ß√£o t√©cnica focando em:
        1. Aspectos t√©cnicos do molde ou pe√ßa
        2. Princ√≠pios de design mostrados
        3. Boas pr√°ticas ilustradas
        4. Poss√≠veis aplica√ß√µes e contextos de uso
        
        Descri√ß√£o:"""
        
        # Obter caminhos das imagens v√°lidas
        image_paths = []
        for file_path in image_dir.rglob("*"):
            if file_path.suffix.lower() in ['.jpg', '.png', '.jpeg']:
                try:
                    # Verificar se a imagem √© v√°lida
                    with Image.open(file_path) as img:
                        img.verify()
                    image_paths.append(str(file_path))
                except Exception as e:
                    print(f"Imagem inv√°lida ignorada {file_path}: {e}")
                    continue
        
        if not image_paths:
            print("Nenhuma imagem v√°lida encontrada")
            return []
            
        # Gerar descri√ß√µes t√©cnicas para cada imagem
        image_descriptions = []
        for image_path in image_paths:
            try:
                file_name = Path(image_path).name
                # Gerar descri√ß√£o t√©cnica usando o LLM
                description_prompt = description_template.format(image_name=file_name)
                description = description_llm.invoke(description_prompt)
                if isinstance(description, str):
                    image_descriptions.append((image_path, description))
                else:
                    # Se a resposta n√£o for string, tentar extrair o conte√∫do
                    description_text = getattr(description, 'content', str(description))
                    image_descriptions.append((image_path, description_text))
            except Exception as e:
                print(f"Erro ao gerar descri√ß√£o para {image_path}: {e}")
                continue
        
        # Preparar o contexto de busca
        search_context = f"""
        Contexto t√©cnico de moldes de inje√ß√£o:
        Pergunta do usu√°rio: {query}
        Resposta t√©cnica: {answer_text}
        """
        
        # Calcular embeddings usando sentence-transformers
        context_embedding = similarity_model.encode(search_context, convert_to_tensor=True)
        
        # Calcular similaridade e relev√¢ncia
        ranked_images = []
        for path, desc in image_descriptions:
            try:
                # Gerar embedding da descri√ß√£o
                desc_embedding = similarity_model.encode(desc, convert_to_tensor=True)
                
                # Calcular similaridade coseno usando torch
                similarity = util.pytorch_cos_sim(context_embedding, desc_embedding).item()
                
                # Normalizar similaridade para [0,1]
                normalized_similarity = float((similarity + 1) / 2)
                
                # Adicionar √† lista apenas se a similaridade for relevante
                if normalized_similarity > 0.3:  # Limiar de relev√¢ncia
                    ranked_images.append((path, desc, normalized_similarity))
            except Exception as e:
                print(f"Erro ao calcular similaridade para {path}: {e}")
                continue
        
        # Ordenar por similaridade e pegar os top N
        ranked_images.sort(key=lambda x: x[2], reverse=True)
        top_images = ranked_images[:n]
        
        # Formatar as descri√ß√µes finais
        formatted_results = []
        for path, desc, score in top_images:
            formatted_desc = f"""
            **Descri√ß√£o T√©cnica do Componente**:
            {desc}
            
            **Contexto de Aplica√ß√£o**:
            - Relev√¢ncia para a consulta: {score:.2%}
            - Tipo de componente: {Path(path).stem}
            """
            formatted_results.append((path, formatted_desc, score))
        
        return formatted_results
        
    except Exception as e:
        print(f"Erro geral na fun√ß√£o describe_and_rank_images: {e}")
        return []



# Sem utilizar sentence treansformers
# from PIL import Image
# import numpy as np
# from typing import List, Tuple, Any, Dict
# import os
# from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_core.documents import Document
# from pathlib import Path

# def describe_and_rank_images(query: str, answer: Any, image_dir: str, n: int = 3) -> List[Tuple[str, str, float]]:
#     """
#     Retorna os caminhos das imagens mais relevantes junto com suas descri√ß√µes e relev√¢ncia.
    
#     Args:
#         query (str): A pergunta do usu√°rio
#         answer (Any): A resposta gerada pelo sistema (pode ser string ou AIMessage)
#         image_dir (str): Diret√≥rio onde est√£o as imagens
#         n (int): N√∫mero m√°ximo de imagens para retornar
        
#     Returns:
#         List[Tuple[str, str, float]]: Lista de tuplas (caminho_imagem, descri√ß√£o, relev√¢ncia)
#     """
#     try:
#         # Extrair texto da resposta se for AIMessage
#         if hasattr(answer, 'content'):
#             answer_text = answer.content
#         else:
#             answer_text = str(answer)

#         # Verificar se o diret√≥rio existe
#         image_dir = Path(image_dir)
#         if not image_dir.exists():
#             print(f"Diret√≥rio de imagens n√£o encontrado: {image_dir}")
#             return []
        
#         # Inicializar modelos
#         description_llm = ChatOpenAI(temperature=0.3, model='gpt-4', openai_api_key=api_key)
#         embeddings_model = OpenAIEmbeddings(api_key=api_key)
        
#         # Template para gerar descri√ß√µes t√©cnicas das imagens
#         description_template = """Voc√™ √© um especialista em moldes de inje√ß√£o pl√°stica.
#         Analise o nome do arquivo da imagem e gere uma descri√ß√£o t√©cnica detalhada do que ela provavelmente representa.
        
#         Nome do arquivo: {image_name}
        
#         Gere uma descri√ß√£o t√©cnica focando em:
#         1. Aspectos t√©cnicos do molde ou pe√ßa
#         2. Princ√≠pios de design mostrados
#         3. Boas pr√°ticas ilustradas
#         4. Poss√≠veis aplica√ß√µes e contextos de uso
        
#         Descri√ß√£o:"""
        
#         # Obter caminhos das imagens v√°lidas
#         image_paths = []
#         for file_path in image_dir.rglob("*"):
#             if file_path.suffix.lower() in ['.jpg', '.png', '.jpeg']:
#                 try:
#                     # Verificar se a imagem √© v√°lida
#                     with Image.open(file_path) as img:
#                         img.verify()
#                     image_paths.append(str(file_path))
#                 except Exception as e:
#                     print(f"Imagem inv√°lida ignorada {file_path}: {e}")
#                     continue
        
#         if not image_paths:
#             print("Nenhuma imagem v√°lida encontrada")
#             return []
            
#         # Gerar descri√ß√µes t√©cnicas para cada imagem
#         image_descriptions = []
#         for image_path in image_paths:
#             try:
#                 file_name = Path(image_path).name
#                 # Gerar descri√ß√£o t√©cnica usando o LLM
#                 description_prompt = description_template.format(image_name=file_name)
#                 description = description_llm.invoke(description_prompt)
#                 if isinstance(description, str):
#                     image_descriptions.append((image_path, description))
#                 else:
#                     # Se a resposta n√£o for string, tentar extrair o conte√∫do
#                     description_text = getattr(description, 'content', str(description))
#                     image_descriptions.append((image_path, description_text))
#             except Exception as e:
#                 print(f"Erro ao gerar descri√ß√£o para {image_path}: {e}")
#                 continue
        
#         # Preparar o contexto de busca
#         search_context = f"""
#         Contexto t√©cnico de moldes de inje√ß√£o:
#         Pergunta do usu√°rio: {query}
#         Resposta t√©cnica: {answer_text}
#         """
        
#         # Calcular embedding do contexto
#         context_embedding = embeddings_model.embed_query(search_context)
        
#         # Calcular similaridade e relev√¢ncia
#         ranked_images = []
#         for path, desc in image_descriptions:
#             try:
#                 # Gerar embedding da descri√ß√£o
#                 desc_embedding = embeddings_model.embed_query(desc)
                
#                 # Calcular similaridade coseno
#                 similarity = np.dot(context_embedding, desc_embedding) / (
#                     np.linalg.norm(context_embedding) * np.linalg.norm(desc_embedding)
#                 )
                
#                 # Normalizar similaridade para [0,1]
#                 normalized_similarity = float((similarity + 1) / 2)
                
#                 # Adicionar √† lista apenas se a similaridade for relevante
#                 if normalized_similarity > 0.3:  # Limiar de relev√¢ncia
#                     ranked_images.append((path, desc, normalized_similarity))
#             except Exception as e:
#                 print(f"Erro ao calcular similaridade para {path}: {e}")
#                 continue
        
#         # Ordenar por similaridade e pegar os top N
#         ranked_images.sort(key=lambda x: x[2], reverse=True)
#         top_images = ranked_images[:n]
        
#         # Formatar as descri√ß√µes finais
#         formatted_results = []
#         for path, desc, score in top_images:
#             formatted_desc = f"""
#             **Descri√ß√£o T√©cnica do Componente**:
#             {desc}
            
#             **Contexto de Aplica√ß√£o**:
#             - Relev√¢ncia para a consulta: {score:.2%}
#             - Tipo de componente: {Path(path).stem}
#             """
#             formatted_results.append((path, formatted_desc, score))
        
#         return formatted_results
        
#     except Exception as e:
#         print(f"Erro geral na fun√ß√£o describe_and_rank_images: {e}")
#         return []





# Inicializa√ß√£o dos retrievers
chunk_retriever = load_chunk_retriever()
summary_retriever = load_summary_retriever()

# Fun√ß√£o para obter caminhos de imagens
def get_image_paths(doc_id: str) -> List[str]:
    logger.info(f"üîç Buscando imagens para documento {doc_id}")
    image_dir = r"/home/romulobrito/projetos/image-RAG/text_image_rag/extracted_images/marco"
    image_paths = []
    for root, _, files in os.walk(image_dir):
        for file in files:
            if file.startswith(doc_id) and file.endswith(('.jpg', '.png')):
                image_paths.append(os.path.join(root, file))
    logger.info(f"üì∏ Encontradas {len(image_paths)} imagens para {doc_id}")
    return image_paths

# Fun√ß√£o para combinar texto e imagens
def combine_text_and_images(context: Dict[str, List[Document]]) -> Dict[str, List[Document]]:
    for key, docs in context.items():
        for doc in docs:
            doc_id = doc.metadata.get("doc_id")
            if doc_id:
                image_paths = get_image_paths(doc_id)
                doc.metadata["image_paths"] = image_paths
                print(f"Added image paths to doc {doc_id}: {image_paths}")  # Adicione este print para depura√ß√£o
    return context


# ====================================================================
# Preprocessing chain
# ====================================================================
class Condense(BaseModel):
    """Sumariza o hist√≥rico do chat e cria uma pergunta independente para recupera√ß√£o RAG sobre moldes de inje√ß√£o"""
    condensed_history: str = Field(description="hist√≥rico do chat sumarizado focado em aspectos t√©cnicos de moldes")
    standalone_question: str = Field(description="pergunta independente condensada do hist√≥rico e pergunta de acompanhamento sobre moldes de inje√ß√£o")

condense_functions = [convert_to_openai_function(Condense)]

condense_template = """Sumarize o hist√≥rico do chat de forma concisa,
e combine-o com a pergunta/solicita√ß√£o de acompanhamento para criar uma pergunta independente para recupera√ß√£o RAG.
Foque apenas em informa√ß√µes t√©cnicas sobre moldes de inje√ß√£o pl√°stica que sejam relevantes para entender a pergunta.
N√£o inclua detalhes desnecess√°rios ou perguntas anteriores que n√£o agreguem ao contexto t√©cnico.
Certifique-se de que a pergunta independente n√£o repita informa√ß√µes j√° respondidas.

Exemplo 1:
Chat History:
User: Quais s√£o os principais tipos de canais de refrigera√ß√£o em moldes?
Assistant: Os principais tipos s√£o canais retos, em cascata e conformais, cada um com suas vantagens espec√≠ficas.
User: Como funcionam os canais conformais?

Condensed History: Usu√°rio perguntou sobre tipos de canais de refrigera√ß√£o, e agora quer entender especificamente os canais conformais.

Standalone Question: Qual √© o princ√≠pio de funcionamento e as vantagens dos canais de refrigera√ß√£o conformais em moldes de inje√ß√£o?

Exemplo 2:
Chat History:
User: Quais s√£o as melhores pr√°ticas para design de extratores?
Assistant: Os extratores devem ser posicionados estrategicamente para evitar marcas e garantir extra√ß√£o uniforme.
User: Como evitar marcas de extrator na pe√ßa?

Condensed History: Ap√≥s discuss√£o sobre design de extratores, usu√°rio busca informa√ß√µes espec√≠ficas sobre preven√ß√£o de marcas.

Standalone Question: Quais s√£o as t√©cnicas e considera√ß√µes de design para minimizar marcas de extrator em pe√ßas injetadas?

Agora, processe o seguinte:

Chat History:
{chat_history}

Follow Up Question/Request: {question}

Forne√ßa tanto o hist√≥rico condensado quanto a pergunta independente, mantendo o foco em aspectos t√©cnicos de moldes de inje√ß√£o.
"""

CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_template)

llm = ChatOpenAI(temperature=0, model='gpt-4o-mini', openai_api_key=api_key)

llm_to_condense = llm.bind(
    functions=condense_functions,
    function_call={"name": "Condense"}
)

# turns nested dict into a flat dict
def format_query(input_dict):
    condensed_info_dict = input_dict.pop('condensed_info')
    input_dict['condensed_history'] = condensed_info_dict['condensed_history']
    input_dict['standalone_question'] = condensed_info_dict['standalone_question']
    return input_dict

preprocess_query_chain = RunnablePassthrough.assign(condensed_info = CONDENSE_QUESTION_PROMPT                                   
                                                    | llm_to_condense 
                                                    | JsonOutputFunctionsParser()) | format_query


# ====================================================================
# Router chain and retrieval chain
# ====================================================================
class RouteQuery1(BaseModel):
    """Route a user query to the most relevant datasource."""
    datasources: List[Literal["summary_store", "vector_store"]] = Field(
        ...,
        description="Given a user question choose which datasources would be most relevant for answering their question",
    )

router_system_prompt1 = """You are an expert at routing user questions about plastic injection mold design to the appropriate data source.
There are two possible destinations:
1. vector_store: This store contains detailed chunks of text about injection mold design.
2. summary_store: This store contains summaries of injection mold design documents.

When deciding where to route a query, consider the following:
- If the query asks for detailed information, specific techniques, or in-depth content about plastic injection mold design, route it to the vector_store.
- If the query asks for an overview, summary, or general information about plastic injection mold design concepts, route it to the summary_store.
- If the query involves both types of information, route it to both stores.

Examples:
1. User query: "What are the basic principles of injection mold design?"
   Routing: summary_store

2. User query: "What are the specific requirements for designing cooling channels in an injection mold?"
   Routing: vector_store

3. User query: "Can you explain the importance of draft angles and provide detailed guidelines for different materials?"
   Routing: summary_store, vector_store
"""

router_prompt1 = ChatPromptTemplate.from_messages(
    [
        ("system", router_system_prompt1),
        ("human", "{standalone_question}"),
    ]
)

router_llm1 = ChatOpenAI(temperature=0, model='gpt-3.5-turbo', openai_api_key=api_key)
#router_llm1 = ChatOpenAI(temperature=0, model='gpt-4o-mini', openai_api_key=api_key)
structured_llm1 = router_llm1.with_structured_output(RouteQuery1)
router_chain1 = RunnablePassthrough.assign(classification1 = (lambda x: x['standalone_question']) | router_prompt1 | structured_llm1)


class RouteQuery2(BaseModel):
    """Route a user query to the most relevant datasource."""
    datasources: List[Literal["marco"]] = Field(
        ...,
        description="Given a user question choose which datasources would be most relevant for answering their question about injection mold design",
    )

router_system_prompt2 = """You are an expert at routing user questions about plastic injection mold design.
The only destination available is:

1. marco: Contains information and resources related to the design and manufacturing of injection molds.

When deciding where to route a query, consider the following:
- If the query is about injection mold design, manufacturing, best practices, or any related topics, route it to "marco".
- All queries related to plastic injection molding should be routed to "marco".

Examples:

1. User query: "What are the best practices in Injection Mold Design?"
    Routing: marco

2. User query: "How to design cooling channels for injection molds?"
    Routing: marco

3. User query: "What are the key considerations for plastic part design?"
    Routing: marco

4. User query: "Can you show me examples of good mold design practices?"
    Routing: marco
"""

router_prompt2 = ChatPromptTemplate.from_messages([
    ("system", router_system_prompt2),
    ("human", "{standalone_question}"),
])

router_llm2 = ChatOpenAI(temperature=0, model='gpt-4o-mini', openai_api_key=api_key)
#router_llm2 = ChatOpenAI(temperature=0, model='gpt-4o', openai_api_key=api_key)
structured_llm2 = router_llm2.with_structured_output(RouteQuery2)
router_chain2 = RunnablePassthrough.assign(classification2 = (lambda x: x['standalone_question']) | router_prompt2 | structured_llm2)

def remove_sub_docs(doc_dict: Dict[str, List[Document]]) -> Dict[str, List[Document]]:

    """Removes sub_docs from the context to save on LLM context window"""

    new_doc_dict = {}

    for key, docs in doc_dict.items():
        new_docs = []
        for doc in docs:
            if 'sub_docs' in doc.metadata:
                # Create a copy of the metadata without sub_docs
                new_metadata = {k: v for k, v in doc.metadata.items() if k != 'sub_docs'}
                new_doc = Document(page_content=doc.page_content, metadata=new_metadata)
                new_docs.append(new_doc)
            else:
                new_docs.append(doc)
        new_doc_dict[key] = new_docs

    return new_doc_dict


def route_query(output):
    """Route the query to the appropriate retrievers based on the classification"""

    # Extract classifications from the output
    classification1 = output.get('classification1')
    classification2 = output.get('classification2')

    # Initialize a list to store selected retrievers
    selected_retrievers = []

    # Map the datasources to retrievers
    datasource_map = {
        'vector_store': chunk_retriever,  # Certifique-se de que isso √© uma inst√¢ncia de CustomParentDocumentRetriever
        'summary_store': summary_retriever  # Certifique-se de que isso √© uma inst√¢ncia de CustomParentDocumentRetriever
    }

    # Helper function to add retrievers based on classification
    def add_retrievers(location, datasources):
        for datasource in datasources:
            retriever = datasource_map.get(datasource)
            if retriever and hasattr(retriever, 'retrieve'):
                selected_retrievers.append((datasource, retriever))

    # Extract and combine the datasources from both classifications
    if classification1:
        datasources = classification1.datasources
    else:
        datasources = []

    if classification2:
        locations = classification2.datasources
    else:
        locations = []

    # Combine the datasources with the specific locations
    for location in locations:
        add_retrievers(location, datasources)

    # Use each selected retriever to get context
    context = {}
    standalone_question = output.get('standalone_question')
    for datasource, retriever in selected_retrievers:
        retrieved_docs = retriever.retrieve(standalone_question)  # Use o m√©todo retrieve
        context[f"{datasource}"] = retrieved_docs

    context = remove_sub_docs(context)

    return context
retrieval_chain = RunnablePassthrough.assign(context = route_query)


# ====================================================================
# QA chain
# ====================================================================
class QAFormat(BaseModel):
    """You are a knowledgeable AI assistant specializing in plastic injection mold design. 
                   Provide detailed and accurate information about best practices, design considerations, and troubleshooting tips for plastic injection molding."""
    answer: str = Field(description="Answer to the user question/request")
    image_paths: List[str] = Field(default_factory=list, description="Relevant image paths extracted from the context. Leave empty if no relevant images are found.")

qaformat_functions = [convert_to_openai_function(QAFormat)]

qa_template = """
Voc√™ √© um especialista em moldes de inje√ß√£o pl√°stica. Responda √† pergunta/solicita√ß√£o baseando-se apenas no contexto fornecido e no hist√≥rico do chat.
Foque em fornecer insights pr√°ticos e melhores pr√°ticas relacionadas ao design de moldes de inje√ß√£o pl√°stica.

Use formata√ß√£o estruturada como pontos ou listas numeradas quando apropriado.

Se houver imagens relevantes mencionadas no contexto, inclua seus caminhos na resposta usando o formato:
[IMAGE_PATH: caminho/para/imagem]

Contexto:
{context}

Hist√≥rico do Chat:
{chat_history}

Pergunta:
{question}

Responda detalhadamente e inclua os caminhos das imagens relevantes encontradas no contexto.
"""

qa_prompt = ChatPromptTemplate.from_messages([
    ("system", "{system_prompt}"),
    ("user", qa_template)
])

qa_llm = ChatOpenAI(temperature=0.5, model='gpt-4', openai_api_key=api_key)
#qa_llm = ChatOpenAI(temperature=0.5, model='gpt-4o', openai_api_key=api_key)
qa_llm_structured = llm.bind(
    functions=qaformat_functions,
    function_call={"name": "QAFormat"}
)

# turns nested dict into a flat dict
def format_qa_out(input_dict):
    """
    Formats the dictionary to move 'answer' and 'image_paths' to the top level.
    
    Args:
        data (dict): The input dictionary containing the nested 'initial_answer' dictionary.
    
    Returns:
        dict: The formatted dictionary with 'answer' renamed to 'initial_answer' and moved to the top level,
              along with 'image_paths'.
    """
    # Extract the nested dictionary
    initial_answer_dict = input_dict.pop('initial_answer')
    
    # Update the original dictionary with the extracted keys
    input_dict['initial_answer'] = initial_answer_dict['answer']
    input_dict['image_paths'] = initial_answer_dict['image_paths']
    
    return input_dict

qa_chain = RunnablePassthrough.assign(initial_answer = qa_prompt | qa_llm_structured | JsonOutputFunctionsParser()) | format_qa_out


# ====================================================================
# Evaluation chain
# ====================================================================
class Evaluate(BaseModel):
    """Avalia se a resposta fornecida sobre moldes de inje√ß√£o √© satisfat√≥ria"""
    eval_result: str = Field(description="Resultado da avalia√ß√£o (Y ou N)")

eval_functions = [convert_to_openai_function(Evaluate)]

eval_template = """Voc√™ √© um avaliador especializado em moldes de inje√ß√£o pl√°stica, respons√°vel por determinar se a resposta fornecida √© satisfat√≥ria.
O sistema responde a consultas t√©cnicas sobre design, manufatura e boas pr√°ticas em moldes de inje√ß√£o.
Considere o hist√≥rico do chat e a pergunta do usu√°rio ao avaliar a resposta.

Uma resposta satisfat√≥ria **n√£o deve** incluir respostas como "N√£o posso responder" ou "N√£o tenho informa√ß√µes suficientes." 
Al√©m disso, a resposta deve ser tecnicamente precisa e relevante para moldes de inje√ß√£o.

Exemplo 1:
User Question:
Quais s√£o os principais tipos de sistemas de refrigera√ß√£o em moldes?

Answer Provided:
Os principais sistemas de refrigera√ß√£o em moldes de inje√ß√£o incluem:
1. Canais convencionais (lineares)
2. Circuitos em s√©rie
3. Canais conformais
4. Sistema baffle
5. Sistema bubbler
Cada sistema tem suas aplica√ß√µes espec√≠ficas dependendo da geometria da pe√ßa e requisitos de resfriamento.

Evaluation: Y

Exemplo 2:
User Question:
Como dimensionar corretamente os canais de alimenta√ß√£o?

Answer Provided:
Desculpe, n√£o tenho informa√ß√µes suficientes sobre dimensionamento de canais de alimenta√ß√£o.

Evaluation: N

Exemplo 3:
User Question:
Mostre exemplos de boas pr√°ticas em extra√ß√£o de pe√ßas.

Answer Provided:
Aqui est√£o algumas boas pr√°ticas para extra√ß√£o de pe√ßas em moldes de inje√ß√£o:
1. Posicionamento adequado dos extratores
2. Uso de √¢ngulos de sa√≠da apropriados
3. Acabamento superficial adequado
[IMAGE_PATH: extratores_exemplo.jpg]

Evaluation: Y

Chat History:
{condensed_history}

User Question:
{question}

Answer Provided:
{initial_answer}

Responda com "Y" se a resposta for satisfat√≥ria (tecnicamente precisa e completa) ou "N" se n√£o for adequada.
"""

EVAL_QUESTION_PROMPT = PromptTemplate.from_template(eval_template)

llm = ChatOpenAI(temperature=0, model='gpt-4o-mini', openai_api_key=api_key)
#llm = ChatOpenAI(temperature=0, model='gpt-4o', openai_api_key=api_key)

llm_to_eval = llm.bind(
    functions=eval_functions,
    function_call={"name": "Evaluate"}
)

eval_chain = RunnablePassthrough.assign(eval_result = EVAL_QUESTION_PROMPT | llm_to_eval | JsonKeyOutputFunctionsParser(key_name="eval_result"))


# ====================================================================
# External search chain
# ====================================================================
search = DuckDuckGoSearchRun()
tools = [search]
llm = ChatOpenAI(temperature=0, model='gpt-4o-mini', openai_api_key=api_key)
functions = [convert_to_openai_function(f) for f in tools]
llm_to_search = llm.bind(functions=functions, function_call="auto")

search_template = """Voc√™ √© um especialista em moldes de inje√ß√£o pl√°stica.
Se a resposta n√£o puder ser encontrada no contexto fornecido, use a ferramenta DuckDuckGoSearch para encontrar informa√ß√µes t√©cnicas relevantes.

Foque em:
- Pr√°ticas recomendadas de design
- Aspectos t√©cnicos de moldes
- Solu√ß√µes pr√°ticas para problemas comuns

Hist√≥rico do Chat:
{chat_history}

Pergunta do Usu√°rio:
{question}

Forne√ßa uma resposta t√©cnica e precisa, evitando informa√ß√µes gen√©ricas.
"""

SEARCH_QUESTION_PROMPT = ChatPromptTemplate.from_messages([
    ("user", search_template),
    #("placeholder", "{agent_scratchpad}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

search_chain = SEARCH_QUESTION_PROMPT | llm_to_search | OpenAIFunctionsAgentOutputParser()
agent_chain = RunnablePassthrough.assign(
    agent_scratchpad= lambda x: format_to_openai_functions(x["intermediate_steps"])
) | search_chain
agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, max_iterations=2)

def external_search_router(eval_output):
    eval_result = eval_output['eval_result']
    logger.info(f"üîÑ Router de busca externa - Avalia√ß√£o: {eval_result}")
    if eval_result == "N":
        logger.info("üåê Iniciando busca externa...")
        return agent_executor | (lambda x: x["output"])
    else:
        logger.info("‚úÖ Usando resposta inicial...")
        return eval_output['initial_answer']

external_search_chain = RunnableLambda(external_search_router)


# Determine the width based on the number of images
def get_image_width(num_images):
    if num_images == 1:
        return 400
    elif num_images == 2:
        return 300
    else:
        return 200
